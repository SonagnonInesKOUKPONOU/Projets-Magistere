# -*- coding: utf-8 -*-
"""Project_3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d5G9w2x9-Ro5p_iJLh3auBCYMqxb1BpU

## Project to computer vision ##

For this project, we will do computer vision. The goal is to understand (i) how machines store images, (ii) how it can pre-process and build handcrafted features, (iii) implement a classifier using these features.

The dataset corresponds to ultrasound images of breasts for identifying any abnormalities.

1. Preprocessing of both dataset
"""

import numpy as np
d = np.load("/content/breastmnist_128.npz")
print(d.files)
for k in d.files:
    a = d[k]
    print(f"{k:15s}", a.shape, a.dtype)

import numpy as np
h = np.load("/content/breastmnist_224.npz")
print(d.files)
for k in d.files:
    a = d[k]
    print(f"{k:15s}", a.shape, a.dtype)

X_train, y_train = d["train_images"], d["train_labels"].ravel()
X_val, y_val     = d["val_images"], d["val_labels"].ravel()
X_test, y_test   = d["test_images"], d["test_labels"].ravel()

print(X_train.shape, y_train.shape)
print(X_val.shape, y_val.shape)
print(X_test.shape, y_test.shape)

"""## PART 1 - VISUALIZATION OF THE DATA ##

In this part, we will visualize some images in the dataset and try to understand what the machine sees
"""

import matplotlib.pyplot as plt

i_healthy = np.where(y_train == 0)[0][0]   # img normal
i_tumor   = np.where(y_train == 1)[0][0]   # image tumorous

# plotting
plt.figure(figsize=(8,4))

plt.subplot(1,2,1)
plt.imshow(X_train[i_healthy], cmap="gray")
plt.title("Healthy (label = 0)")
plt.axis("off")

plt.subplot(1,2,2)
plt.imshow(X_train[i_tumor], cmap="gray")
plt.title("Tumor (label = 1)")
plt.axis("off")

plt.tight_layout()
plt.show()

# What we see when we look at an image
i_healthy = np.where(y_train == 0)[0][1]   # image saine
i_tumor   = np.where(y_train == 1)[0][1]   # image avec tumeur

# Affichage des images
plt.figure(figsize=(8,4))

plt.subplot(1,2,1)
plt.imshow(X_train[i_healthy], cmap="gray")
plt.title("Healthy (label = 0)")
plt.axis("off")

plt.subplot(1,2,2)
plt.imshow(X_train[i_tumor], cmap="gray")
plt.title("Tumor (label = 1)")
plt.axis("off")

plt.tight_layout()
plt.show()

# Now we are looking at the distribution of values for each channel

img = X_train[0]
print("Image shape:", img.shape)

# distribution of values (0-255)
plt.figure(figsize=(6,4))
plt.hist(img.ravel(), bins=256, color='gray', alpha=0.7)
plt.title("Pixel Intensity Distribution (Grayscale)")
plt.xlabel("Pixel intensity (0 = black, 255 = white)")
plt.ylabel("Frequency")
plt.show()

"""As you can see, we denote a certain asymetrie on the graphic. The histogram is concentrated on the left, between pixel intensity of 50 and 150. Which means that the majority of the image is in gray. It will be good to apply in a preprocessing step the normalisation in order to improve the lisibility of the tumor.

## PART 2 - PREPROCESSING ##
"""

# Data Augmentation
from PIL import Image, ImageEnhance, ImageFilter

# SÃ©lectionner une image du jeu d'entraÃ®nement (niveau de gris)
img = Image.fromarray(X_train[0].astype(np.uint8))  # pas besoin de multiplier par 255, dÃ©jÃ  uint8

# Appliquer plusieurs augmentations mÃ©dicalement pertinentes
img_flip = img.transpose(Image.FLIP_LEFT_RIGHT)                      # symÃ©trie horizontale
img_rot  = img.rotate(10, resample=Image.BILINEAR)                   # rotation douce Â±10Â°
img_brt  = ImageEnhance.Brightness(img).enhance(1.15)                # lÃ©gÃ¨re variation de luminositÃ©
img_con  = ImageEnhance.Contrast(img).enhance(1.1)                   # lÃ©ger contraste
img_noisy = img.filter(ImageFilter.GaussianBlur(radius=1))           # simule du lÃ©ger bruit

# Visualiser les augmentations
imgs = [
    (img, "Original"),
    (img_flip, "Flip horizontal"),
    (img_rot, "Rotation +10Â°"),
    (img_brt, "Brightness Ã—1.15"),
    (img_con, "Contrast Ã—1.1"),
    (img_noisy, "Gaussian blur (Ïƒâ‰ˆ1)")
]

plt.figure(figsize=(12,6))
for i, (im, title) in enumerate(imgs):
    plt.subplot(2, 3, i+1)
    plt.imshow(im, cmap="gray")
    plt.title(title)
    plt.axis("off")
plt.tight_layout()
plt.show()

import numpy as np
from PIL import Image, ImageEnhance
import cv2

# ==============================
# 1ï¸âƒ£ DATA AUGMENTATION
# ==============================
def augment_dataset(X, y, do_augmentation=True):
    if not do_augmentation:
        return X, y

    X_aug, y_aug = [], []
    for img, label in zip(X, y):
        img = Image.fromarray((img * 255).astype(np.uint8))

        # Transformations simples
        img_flp = img.transpose(Image.FLIP_LEFT_RIGHT)
        img_rot = img.rotate(10, resample=Image.BILINEAR)
        img_brt = ImageEnhance.Brightness(img).enhance(1.15)
        img_con = ImageEnhance.Contrast(img).enhance(1.1)

        # Convertir et normaliser temporairement entre 0â€“255
        imgs_aug = [img, img_flp, img_rot, img_brt, img_con]

        for im in imgs_aug:
            X_aug.append(np.array(im))
            y_aug.append(label)

    return np.array(X_aug), np.array(y_aug)

# ==============================
# 2ï¸âƒ£ CLAHE
# ==============================
def apply_clahe(X, do_clahe=True):
    if not do_clahe:
        return X
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
    X_clahe = [clahe.apply(img.astype(np.uint8)) for img in X]
    return np.array(X_clahe)

# ==============================
# 3ï¸âƒ£ DENOISING
# ==============================
def denoise_images(X, do_denoise=True):
    if not do_denoise:
        return X
    X_denoised = [cv2.medianBlur(img.astype(np.uint8), 3) for img in X]
    return np.array(X_denoised)

# ==============================
# 4ï¸âƒ£ SCALING
# ==============================
def scale_images(X, do_scaling=True):
    if not do_scaling:
        return X
    return X / 255.0 if X.max() > 1 else X

# ==============================
# 5ï¸âƒ£ NORMALISATION
# ==============================
def normalize_images(X, do_normalize=True):
    if not do_normalize:
        return X
    mean = np.mean(X)
    std = np.std(X)
    return (X - mean) / std

# ==============================
# ðŸ§  PIPELINE GLOBAL
# ==============================
def preprocess_pipeline(X, y,
                        augment=True,
                        clahe=True,
                        denoise=True,
                        scale=True,
                        normalize=True):

    X_prep, y_prep = X.copy(), y.copy()

    # 1ï¸âƒ£ Data augmentation
    X_prep, y_prep = augment_dataset(X_prep, y_prep, do_augmentation=augment)
    print(f"AprÃ¨s augmentation : {X_prep.shape}")

    # 2ï¸âƒ£ CLAHE
    X_prep = apply_clahe(X_prep, do_clahe=clahe)
    print("CLAHE appliquÃ©")

    # 3ï¸âƒ£ Denoising
    X_prep = denoise_images(X_prep, do_denoise=denoise)
    print("Denoising appliquÃ©")

    # 4ï¸âƒ£ Scaling
    X_prep = scale_images(X_prep, do_scaling=scale)
    print(f"AprÃ¨s scaling : min={X_prep.min():.3f}, max={X_prep.max():.3f}")

    # 5ï¸âƒ£ Normalisation
    X_prep = normalize_images(X_prep, do_normalize=normalize)
    print(f"AprÃ¨s normalisation : mean={np.mean(X_prep):.3f}, std={np.std(X_prep):.3f}")

    return X_prep, y_prep

"""## PART 3 - FEATURE ENGINEERING"""

# --- Edge detection (Canny) adapted for BreastMNIST ---
import matplotlib.pyplot as plt
from skimage import feature

# SÃ©lection d'exemples : une image saine et une image anormale (tumeur)
i_safe = np.where(y_train == 0)[0][0]
i_tumor = np.where(y_train == 1)[0][0]

# Extraction des images
safe_img = X_train[i_safe]
tumor_img = X_train[i_tumor]

# Si les images ne sont pas normalisÃ©es entre 0â€“1, on les convertit
if safe_img.max() > 1:
    safe_img = safe_img / 255.0
    tumor_img = tumor_img / 255.0

# DÃ©tection des contours avec Canny
safe_edges = feature.canny(safe_img, sigma=1.5)
tumor_edges = feature.canny(tumor_img, sigma=1.5)

# Visualisation
plt.figure(figsize=(10,4))

plt.subplot(1,2,1)
plt.imshow(safe_img, cmap="gray")
plt.axis("off")
plt.title("Original (Healthy)")

plt.subplot(1,2,2)
plt.imshow(safe_edges, cmap="gray")
plt.axis("off")
plt.title("Canny Edges (Healthy)")
plt.tight_layout()
plt.show()

plt.figure(figsize=(10,4))

plt.subplot(1,2,1)
plt.imshow(tumor_img, cmap="gray")
plt.axis("off")
plt.title("Original (Tumor)")

plt.subplot(1,2,2)
plt.imshow(tumor_edges, cmap="gray")
plt.axis("off")
plt.title("Canny Edges (Tumor)")
plt.tight_layout()
plt.show()

# --- Corner detection (Harris) adapted for BreastMNIST ---
from skimage import feature
import matplotlib.pyplot as plt

# SÃ©lection dâ€™exemples : une image saine et une image tumorale
i_safe = np.where(y_train == 0)[0][0]
i_tumor = np.where(y_train == 1)[0][0]

# Extraction des images
safe_img = X_train[i_safe]
tumor_img = X_train[i_tumor]

# Si les images ne sont pas normalisÃ©es entre 0â€“1
if safe_img.max() > 1:
    safe_img = safe_img / 255.0
    tumor_img = tumor_img / 255.0

# --- Harris corner response ---
safe_hresp = feature.corner_harris(safe_img, method='k', k=0.05)
safe_pts   = feature.corner_peaks(safe_hresp, min_distance=4, threshold_rel=0.02)

tumor_hresp = feature.corner_harris(tumor_img, method='k', k=0.05)
tumor_pts   = feature.corner_peaks(tumor_hresp, min_distance=4, threshold_rel=0.02)

# --- Visualisation ---
plt.figure(figsize=(10,4))

plt.subplot(1,2,1)
plt.imshow(safe_img, cmap="gray")
plt.axis("off")
plt.title(f"Harris corners (Healthy) â€” {len(safe_pts)} pts")
if len(safe_pts):
    plt.scatter(safe_pts[:,1], safe_pts[:,0], s=25, c='r', marker='o')

plt.subplot(1,2,2)
plt.imshow(tumor_img, cmap="gray")
plt.axis("off")
plt.title(f"Harris corners (Tumor) â€” {len(tumor_pts)} pts")
if len(tumor_pts):
    plt.scatter(tumor_pts[:,1], tumor_pts[:,0], s=25, c='r', marker='o')

plt.tight_layout()
plt.show()

# --- Histogram of Oriented Gradients (HOG) for BreastMNIST ---
from skimage.feature import hog
from skimage import transform
import matplotlib.pyplot as plt

# SÃ©lection d'exemples : une image saine et une image tumorale
i_safe = np.where(y_train == 0)[0][0]
i_tumor = np.where(y_train == 1)[0][0]

# Extraction des images
safe_img = X_train[i_safe]
tumor_img = X_train[i_tumor]

# Normalisation si besoin
if safe_img.max() > 1:
    safe_img = safe_img / 255.0
    tumor_img = tumor_img / 255.0

# Redimensionnement (HOG attend souvent 128x128)
safe_in = transform.resize(safe_img, (128,128), anti_aliasing=True)
tumor_in = transform.resize(tumor_img, (128,128), anti_aliasing=True)

# --- Calcul des HOG features et visualisation ---
safe_vec, safe_vis = hog(
    safe_in,
    pixels_per_cell=(8,8),
    cells_per_block=(2,2),
    orientations=9,
    visualize=True,
    block_norm='L2-Hys'
)

tumor_vec, tumor_vis = hog(
    tumor_in,
    pixels_per_cell=(8,8),
    cells_per_block=(2,2),
    orientations=9,
    visualize=True,
    block_norm='L2-Hys'
)

print("HOG feature vector length (Healthy):", safe_vec.shape[0])
print("HOG feature vector length (Tumor):", tumor_vec.shape[0])

# --- Affichage ---
plt.figure(figsize=(10,4))

plt.subplot(1,2,1)
plt.imshow(safe_in, cmap="gray")
plt.axis("off")
plt.title("Input (Healthy)")

plt.subplot(1,2,2)
plt.imshow(safe_vis, cmap="gray")
plt.axis("off")
plt.title("HOG visualization (Healthy)")

plt.tight_layout()
plt.show()

plt.figure(figsize=(10,4))

plt.subplot(1,2,1)
plt.imshow(tumor_in, cmap="gray")
plt.axis("off")
plt.title("Input (Tumor)")

plt.subplot(1,2,2)
plt.imshow(tumor_vis, cmap="gray")
plt.axis("off")
plt.title("HOG visualization (Tumor)")

plt.tight_layout()
plt.show()

"""## PART 4 - MODELING ##

In this part, we are going to build a classifier using handcrafted features
"""

# --- Import des librairies ---
from sklearn.preprocessing import StandardScaler
from sklearn.svm import LinearSVC
from sklearn import metrics
import numpy as np

# PrÃ©traitement complet
X_train_prep, y_train_prep = preprocess_pipeline(X_train, y_train,
                                                 augment=True,
                                                 clahe=False,
                                                 denoise=False,
                                                 scale=True,
                                                 normalize=True)

X_val_prep, y_val_prep = preprocess_pipeline(X_val, y_val,
                                             augment=False,
                                             clahe=False,
                                             denoise=False,
                                             scale=True,
                                             normalize=False)
X_test_prep, y_test_prep = preprocess_pipeline(X_test, y_test,
                                               augment=False,
                                               clahe=False,
                                             denoise=False,
                                             scale=True,
                                             normalize=False)


# Mise Ã  plat + entraÃ®nement
X_train_flat = X_train_prep.reshape(len(X_train_prep), -1)
X_val_flat   = X_val_prep.reshape(len(X_val_prep), -1)
X_test_flat  = X_test_prep.reshape(len(X_test_prep), -1)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_flat)
X_val_scaled   = scaler.transform(X_val_flat)
X_test_scaled  = scaler.transform(X_test_flat)

# -------- SVM MODEL ---------
from sklearn import metrics
from sklearn.svm import LinearSVC
import pandas as pd
import matplotlib.pyplot as plt

# --- EntraÃ®nement du modÃ¨le ---
clf = LinearSVC(C=1.0, max_iter=8000, random_state=42)
clf.fit(X_train_scaled, y_train_prep)

# --- PrÃ©diction et "scores" continus ---
pred_test = clf.predict(X_test_scaled)
scores = clf.decision_function(X_test_scaled)  # ðŸ”¹ utilisÃ© pour AUC

# --- Calcul de l'AUC ---
auc = metrics.roc_auc_score(y_test_prep, scores)
acc = metrics.accuracy_score(y_test_prep, pred_test)
f1 = metrics.f1_score(y_test_prep, pred_test)

print(f"\nâœ… Test accuracy: {acc:.3f}")
print(f"ðŸ AUC (Linear SVM): {auc:.3f}")
print(f"ðŸŽ¯ F1-score: {f1:.3f}")

# --- Rapport complet ---
print(metrics.classification_report(y_test_prep, pred_test, digits=3))

# --- Matrice de confusion ---
cm = metrics.confusion_matrix(y_test_prep, pred_test)
print("\nConfusion Matrix:\n", pd.DataFrame(cm,
                                            index=["true_SAFE", "true_SICK"],
                                            columns=["pred_SAFE", "pred_SICK"]))

# --- Courbe ROC ---
fpr, tpr, _ = metrics.roc_curve(y_test_prep, scores)
plt.figure(figsize=(6,5))
plt.plot(fpr, tpr, label=f"AUC = {auc:.3f}", color='darkorange')
plt.plot([0,1], [0,1], 'k--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve â€” LinearSVC (Decision Function)")
plt.legend()
plt.show()

# ------- LOGISTIQ MODEL --------
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
import pandas as pd
import matplotlib.pyplot as plt

# --- EntraÃ®nement ---
logreg = LogisticRegression(max_iter=8000, random_state=42)
logreg.fit(X_train_scaled, y_train_prep)

# --- Ã‰valuation sur test ---
pred_test = logreg.predict(X_test_scaled)
proba_test = logreg.predict_proba(X_test_scaled)[:, 1]  # probabilitÃ© de la classe "1"

print("\n Test accuracy:", metrics.accuracy_score(y_test_prep, pred_test))
print(metrics.classification_report(y_test_prep, pred_test, digits=3))

# --- Matrice de confusion ---
cm = metrics.confusion_matrix(y_test_prep, pred_test)
print("Confusion Matrix:\n", pd.DataFrame(cm,
                                          index=["true_SAFE", "true_SICK"],
                                          columns=["pred_SAFE", "pred_SICK"]))

# --- AUC / ROC ---
fpr, tpr, thresholds = metrics.roc_curve(y_test_prep, proba_test)
auc_score = metrics.roc_auc_score(y_test_prep, proba_test)
print(f"\n AUC (Logistic Regression): {auc_score:.3f}")

plt.figure(figsize=(6,5))
plt.plot(fpr, tpr, label=f"AUC = {auc_score:.3f}")
plt.plot([0,1], [0,1], 'k--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve â€” Logistic Regression")
plt.legend()
plt.show()

# ============================================================
# ðŸ“¦ IMPORTS
# ============================================================
import numpy as np
from skimage import feature, transform
from sklearn.preprocessing import StandardScaler
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
import matplotlib.pyplot as plt
import pandas as pd

# ============================================================
# 1ï¸âƒ£ EXTRACTION DES FEATURES (HARRIS)
# ============================================================
def extract_harris_features(X, target_size=128, k=0.05, min_distance=4, threshold_rel=0.02):
    """
    Calcule une carte binaire de coins Harris pour chaque image
    et la redimensionne Ã  target_size.
    """
    X_feat = []
    for i in range(len(X)):
        img = transform.resize(X[i], (target_size, target_size), anti_aliasing=True)
        hresp = feature.corner_harris(img, method='k', k=k)
        pts = feature.corner_peaks(hresp, min_distance=min_distance, threshold_rel=threshold_rel)
        feat = np.zeros_like(img)
        if len(pts) > 0:
            feat[pts[:, 0], pts[:, 1]] = 1.0  # Marque les coins dÃ©tectÃ©s
        X_feat.append(feat)
    return np.array(X_feat, dtype="float32")

print("â³ Extraction des features HARRIS...")
X_train_harris = extract_harris_features(X_train_prep)
X_val_harris   = extract_harris_features(X_val_prep)
X_test_harris  = extract_harris_features(X_test_prep)
print("âœ… Done. Shapes:", X_train_harris.shape, X_val_harris.shape, X_test_harris.shape)

y_train = y_train_prep
y_val   = y_val_prep
y_test  = y_test_prep

# ============================================================
# 2ï¸âƒ£ FLATTEN + SCALING
# ============================================================
X_train_flat = X_train_harris.reshape(len(X_train_harris), -1)
X_val_flat   = X_val_harris.reshape(len(X_val_harris), -1)
X_test_flat  = X_test_harris.reshape(len(X_test_harris), -1)

scaler = StandardScaler()
X_train_s = scaler.fit_transform(X_train_flat)
X_val_s   = scaler.transform(X_val_flat)
X_test_s  = scaler.transform(X_test_flat)

# ============================================================
# 3ï¸âƒ£ SVM LINEAIRE
# ============================================================
clf_svm = LinearSVC(C=1.0, max_iter=8000, random_state=42)
clf_svm.fit(X_train_s, y_train)

pred_val_svm = clf_svm.predict(X_val_s)
pred_test_svm = clf_svm.predict(X_test_s)

print("\nðŸ”¹ [SVM - HARRIS] Validation Accuracy:", metrics.accuracy_score(y_test, pred_test_svm))
print(metrics.classification_report(y_test, pred_test_svm, digits=3))

proba_test_svm = clf_svm.decision_function(X_test_s)
auc_svm = metrics.roc_auc_score(y_test, proba_test_svm)
print(f"ðŸ [SVM - HARRIS] Test AUC: {auc_svm:.3f}")

cm_svm = metrics.confusion_matrix(y_test, pred_test_svm)
print("Confusion Matrix (SVM-Harris):\n", pd.DataFrame(cm_svm,
                                                     index=["true_SAFE","true_SICK"],
                                                     columns=["pred_SAFE","pred_SICK"]))

# ============================================================
# 4ï¸âƒ£ RÃ‰GRESSION LOGISTIQUE
# ============================================================
clf_log = LogisticRegression(max_iter=5000, solver='liblinear', random_state=42)
clf_log.fit(X_train_s, y_train)

pred_val_log = clf_log.predict(X_val_s)
pred_test_log = clf_log.predict(X_test_s)
proba_test_log = clf_log.predict_proba(X_test_s)[:,1]

print("\nðŸ”¹ [LOGISTIC - HARRIS] Validation Accuracy:", metrics.accuracy_score(y_test, pred_test_log))
print(metrics.classification_report(y_test, pred_test_log, digits=3))

auc_log = metrics.roc_auc_score(y_test, proba_test_log)
print(f"ðŸ [LOGISTIC - HARRIS] Test AUC: {auc_log:.3f}")

cm_log = metrics.confusion_matrix(y_test, pred_test_log)
print("Confusion Matrix (Logistic-Harris):\n", pd.DataFrame(cm_log,
                                                          index=["true_SAFE","true_SICK"],
                                                          columns=["pred_SAFE","pred_SICK"]))

# ============================================================
# 5ï¸âƒ£ ROC COMPARATIVE
# ============================================================
fpr_svm, tpr_svm, _ = metrics.roc_curve(y_test, proba_test_svm)
fpr_log, tpr_log, _ = metrics.roc_curve(y_test, proba_test_log)

plt.figure(figsize=(6,5))
plt.plot(fpr_svm, tpr_svm, label=f"SVM (AUC={auc_svm:.3f})", color='royalblue')
plt.plot(fpr_log, tpr_log, label=f"Logistic (AUC={auc_log:.3f})", color='darkorange')
plt.plot([0,1], [0,1], 'k--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curves â€” Harris Features")
plt.legend()
plt.show()

# ============================================================
# ðŸ“¦ IMPORTS
# ============================================================
import numpy as np
from skimage.feature import hog
from skimage import transform
from sklearn.preprocessing import StandardScaler
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
import matplotlib.pyplot as plt
import pandas as pd

# ============================================================
# 1ï¸âƒ£ EXTRACTION DES DESCRIPTEURS HOG
# ============================================================
def extract_hog_features(X, target_size=128):
    """Calcule les descripteurs HOG pour un ensemble d'images"""
    X_hog = []
    for i in range(len(X)):
        img = transform.resize(X[i], (target_size, target_size), anti_aliasing=True)
        vec = hog(img,
                  pixels_per_cell=(8,8),
                  cells_per_block=(2,2),
                  orientations=9,
                  block_norm='L2-Hys',
                  feature_vector=True)
        X_hog.append(vec)
    return np.array(X_hog, dtype="float32")

print("â³ Extraction des features HOG...")
X_train_hog = extract_hog_features(X_train_prep)
X_val_hog   = extract_hog_features(X_val_prep)
X_test_hog  = extract_hog_features(X_test_prep)
print("âœ… Done. Shapes:", X_train_hog.shape, X_val_hog.shape, X_test_hog.shape)

y_train = y_train_prep
y_val   = y_val_prep
y_test  = y_test_prep

# ============================================================
# 2ï¸âƒ£ SCALING
# ============================================================
scaler = StandardScaler()
X_train_s = scaler.fit_transform(X_train_hog)
X_val_s   = scaler.transform(X_val_hog)
X_test_s  = scaler.transform(X_test_hog)

# ============================================================
# 3ï¸âƒ£ SVM LINEAIRE
# ============================================================
clf_svm = LinearSVC(C=1.0, max_iter=8000, random_state=42)
clf_svm.fit(X_train_s, y_train)

pred_val_svm = clf_svm.predict(X_val_s)
pred_test_svm = clf_svm.predict(X_test_s)

print("\nðŸ”¹ [SVM - HOG] Validation Accuracy:", metrics.accuracy_score(y_test, pred_test_svm))
print(metrics.classification_report(y_test, pred_test_svm, digits=3))

# AUC avec decision_function
proba_test_svm = clf_svm.decision_function(X_test_s)
auc_svm = metrics.roc_auc_score(y_test, proba_test_svm)
print(f"ðŸ [SVM - HOG] Test AUC: {auc_svm:.3f}")

cm_svm = metrics.confusion_matrix(y_test, pred_test_svm)
print("Confusion Matrix (SVM-HOG):\n", pd.DataFrame(cm_svm,
                                                   index=["true_SAFE","true_SICK"],
                                                   columns=["pred_SAFE","pred_SICK"]))

# ============================================================
# 4ï¸âƒ£ RÃ‰GRESSION LOGISTIQUE
# ============================================================
clf_log = LogisticRegression(max_iter=5000, solver='liblinear', random_state=42)
clf_log.fit(X_train_s, y_train)

pred_val_log = clf_log.predict(X_val_s)
pred_test_log = clf_log.predict(X_test_s)
proba_test_log = clf_log.predict_proba(X_test_s)[:,1]

print("\nðŸ”¹ [LOGISTIC - HOG] Validation Accuracy:", metrics.accuracy_score(y_test, pred_test_log))
print(metrics.classification_report(y_test, pred_test_log, digits=3))

auc_log = metrics.roc_auc_score(y_test, proba_test_log)
print(f"ðŸ [LOGISTIC - HOG] Test AUC: {auc_log:.3f}")

cm_log = metrics.confusion_matrix(y_test, pred_test_log)
print("Confusion Matrix (Logistic-HOG):\n", pd.DataFrame(cm_log,
                                                       index=["true_SAFE","true_SICK"],
                                                       columns=["pred_SAFE","pred_SICK"]))

# ============================================================
# 5ï¸âƒ£ ROC COMPARATIVE
# ============================================================
fpr_svm, tpr_svm, _ = metrics.roc_curve(y_test, proba_test_svm)
fpr_log, tpr_log, _ = metrics.roc_curve(y_test, proba_test_log)

plt.figure(figsize=(6,5))
plt.plot(fpr_svm, tpr_svm, label=f"SVM (AUC={auc_svm:.3f})", color='royalblue')
plt.plot(fpr_log, tpr_log, label=f"Logistic (AUC={auc_log:.3f})", color='darkorange')
plt.plot([0,1], [0,1], 'k--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curves â€” HOG Features")
plt.legend()
plt.show()

# ============================================================
# ðŸ“¦ IMPORTS
# ============================================================
import numpy as np
from skimage import feature, transform
from sklearn.preprocessing import StandardScaler
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
import pandas as pd
import matplotlib.pyplot as plt

# ============================================================
# 1ï¸âƒ£ APPLICATION DE CANNY SUR CHAQUE ENSEMBLE
# ============================================================
def extract_canny_edges(X, target_size=128, sigma=3):
    """Convert images to edge maps (Canny) and flatten"""
    X_edge = np.empty((len(X), target_size, target_size), dtype="float32")
    for i in range(len(X)):
        e = feature.canny(X[i], sigma=sigma).astype("float32")
        X_edge[i] = transform.resize(e, (target_size, target_size), anti_aliasing=False)
    X_vec = X_edge.reshape(len(X_edge), -1)  # (N, target_size*target_size)
    return X_vec

print("â³ Extraction des bords (Canny)...")
X_train_canny = extract_canny_edges(X_train_prep)
X_val_canny   = extract_canny_edges(X_val_prep)
X_test_canny  = extract_canny_edges(X_test_prep)

print("âœ… Done. Shapes:", X_train_canny.shape, X_val_canny.shape, X_test_canny.shape)

y_train = y_train_prep
y_val   = y_val_prep
y_test  = y_test_prep

# ============================================================
# 2ï¸âƒ£ STANDARDISATION
# ============================================================
scaler = StandardScaler()
X_train_s = scaler.fit_transform(X_train_canny)
X_val_s   = scaler.transform(X_val_canny)
X_test_s  = scaler.transform(X_test_canny)

# ============================================================
# 3ï¸âƒ£ SVM LINEAIRE
# ============================================================
clf_svm = LinearSVC(C=1.0, max_iter=8000, random_state=42)
clf_svm.fit(X_train_s, y_train)

pred_val_svm = clf_svm.predict(X_val_s)
pred_test_svm = clf_svm.predict(X_test_s)

print("\nðŸ”¹ [SVM] Validation Accuracy:", metrics.accuracy_score(y_test, pred_test_svm))
print(metrics.classification_report(y_test, pred_test_svm, digits=3))

# AUC avec decision_function
proba_test_svm = clf_svm.decision_function(X_test_s)
auc_svm = metrics.roc_auc_score(y_test, proba_test_svm)
print(f"ðŸ [SVM] Test AUC: {auc_svm:.3f}")

cm_svm = metrics.confusion_matrix(y_test, pred_test_svm)
print("Confusion Matrix (SVM):\n", pd.DataFrame(cm_svm,
                                               index=["true_SAFE","true_SICK"],
                                               columns=["pred_SAFE","pred_SICK"]))

# ============================================================
# 4ï¸âƒ£ RÃ‰GRESSION LOGISTIQUE
# ============================================================
clf_log = LogisticRegression(max_iter=5000, solver='liblinear', random_state=42)
clf_log.fit(X_train_s, y_train)

pred_val_log = clf_log.predict(X_val_s)
pred_test_log = clf_log.predict(X_test_s)
proba_test_log = clf_log.predict_proba(X_test_s)[:,1]

print("\nðŸ”¹ [LOGISTIC] Validation Accuracy:", metrics.accuracy_score(y_test, pred_test_log))
print(metrics.classification_report(y_test, pred_test_log, digits=3))

auc_log = metrics.roc_auc_score(y_test, proba_test_log)
print(f"ðŸ [LOGISTIC] Test AUC: {auc_log:.3f}")

cm_log = metrics.confusion_matrix(y_test, pred_test_log)
print("Confusion Matrix (Logistic):\n", pd.DataFrame(cm_log,
                                                   index=["true_SAFE","true_SICK"],
                                                   columns=["pred_SAFE","pred_SICK"]))

# ============================================================
# 5ï¸âƒ£ COURBES ROC (Comparaison)
# ============================================================
fpr_svm, tpr_svm, _ = metrics.roc_curve(y_test, proba_test_svm)
fpr_log, tpr_log, _ = metrics.roc_curve(y_test, proba_test_log)

plt.figure(figsize=(6,5))
plt.plot(fpr_svm, tpr_svm, label=f"SVM (AUC={auc_svm:.3f})", color='royalblue')
plt.plot(fpr_log, tpr_log, label=f"Logistic (AUC={auc_log:.3f})", color='darkorange')
plt.plot([0,1], [0,1], 'k--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curves â€” Canny Features")
plt.legend()
plt.show()

# ----------- HARRIS + HOC combined -----------------

from sklearn.preprocessing import StandardScaler
from sklearn.svm import LinearSVC
from sklearn.metrics import classification_report, roc_curve, auc
import matplotlib.pyplot as plt
import numpy as np

# --------------------------
# 1ï¸âƒ£ Combinaison des features
# --------------------------
# Harris a une sortie 3D, on l'aplatit
X_train_harris_flat = X_train_harris.reshape(X_train_harris.shape[0], -1)
X_val_harris_flat  = X_val_harris.reshape(X_val_harris.shape[0], -1)
X_test_harris_flat  = X_test_harris.reshape(X_test_harris.shape[0], -1)


X_train_combined = np.concatenate([X_train_harris_flat, X_train_hog], axis=1)
X_val_combined  = np.concatenate([X_val_harris_flat,  X_val_hog], axis=1)
X_test_combined  = np.concatenate([X_test_harris_flat,  X_test_hog], axis=1)


print("Combined features shape (train):", X_train_combined.shape)
print("Combined features shape (test): ", X_val_combined.shape)
print("Combined features shape (test): ", X_test_combined.shape)

# --------------------------
# 2ï¸âƒ£ Scaling
# --------------------------
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_combined)
X_val_scaled  = scaler.transform(X_val_combined)
X_test_scaled  = scaler.transform(X_test_combined)

y_train = y_train_prep
y_val   = y_val_prep
y_test  = y_test_prep

# ============================================================
# 3ï¸âƒ£ SVM LINEAIRE
# ============================================================
clf_svm = LinearSVC(C=1.0, max_iter=8000, random_state=42)
clf_svm.fit(X_train_scaled, y_train)

pred_val_svm = clf_svm.predict(X_val_scaled)
pred_test_svm = clf_svm.predict(X_test_scaled)

print("\nðŸ”¹ [SVM - HOG&Harris] Accuracy:", metrics.accuracy_score(y_test, pred_test_svm))
print(metrics.classification_report(y_test, pred_test_svm, digits=3))

# AUC avec decision_function
proba_test_svm = clf_svm.decision_function(X_test_scaled)
auc_svm = metrics.roc_auc_score(y_test, proba_test_svm)
print(f"ðŸ [SVM - HOG&Harris] Test AUC: {auc_svm:.3f}")

cm_svm = metrics.confusion_matrix(y_test, pred_test_svm)
print("Confusion Matrix (SVM-HOG):\n", pd.DataFrame(cm_svm,
                                                   index=["true_SAFE","true_SICK"],
                                                   columns=["pred_SAFE","pred_SICK"]))

# ============================================================
# 4ï¸âƒ£ RÃ‰GRESSION LOGISTIQUE
# ============================================================
clf_log = LogisticRegression(max_iter=5000, solver='liblinear', random_state=42)
clf_log.fit(X_train_scaled, y_train)

pred_val_log = clf_log.predict(X_val_scaled)
pred_test_log = clf_log.predict(X_test_scaled)
proba_test_log = clf_log.predict_proba(X_test_scaled)[:,1]

print("\nðŸ”¹ [LOGISTIC - HOG&Harris]  Accuracy:", metrics.accuracy_score(y_test, pred_test_log))
print(metrics.classification_report(y_test, pred_test_log, digits=3))

auc_log = metrics.roc_auc_score(y_test, proba_test_log)
print(f"ðŸ [LOGISTIC - HOG&Harris] Test AUC: {auc_log:.3f}")

cm_log = metrics.confusion_matrix(y_test, pred_test_log)
print("Confusion Matrix (Logistic-HOG&Harris):\n", pd.DataFrame(cm_log,
                                                       index=["true_SAFE","true_SICK"],
                                                       columns=["pred_SAFE","pred_SICK"]))

# ============================================================
# 5ï¸âƒ£ ROC COMPARATIVE
# ============================================================
fpr_svm, tpr_svm, _ = metrics.roc_curve(y_test, proba_test_svm)
fpr_log, tpr_log, _ = metrics.roc_curve(y_test, proba_test_log)

plt.figure(figsize=(6,5))
plt.plot(fpr_svm, tpr_svm, label=f"SVM (AUC={auc_svm:.3f})", color='royalblue')
plt.plot(fpr_log, tpr_log, label=f"Logistic (AUC={auc_log:.3f})", color='darkorange')
plt.plot([0,1], [0,1], 'k--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curves â€” HOG Features & Harris Feature")
plt.legend()
plt.show()

import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models, callbacks
from tensorflow.keras.optimizers import Adam
from sklearn.utils.class_weight import compute_class_weight
from sklearn import metrics
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# ----------------------------
# PrÃ©paration des donnÃ©es
# ----------------------------
X_train, y_train = d["train_images"], d["train_labels"].ravel()
X_val, y_val     = d["val_images"], d["val_labels"].ravel()
X_test, y_test   = d["test_images"], d["test_labels"].ravel()

y_train = y_train.astype(int).ravel()
y_val   = y_val.astype(int).ravel()
y_test  = y_test.astype(int).ravel()


# -----------------------------
# 1ï¸âƒ£ Augmentation OFFLINE (statique)
# -----------------------------
def augment_image(img_array):
    """Applique plusieurs transformations sur une image (PIL)."""
    img = Image.fromarray(img_array.astype(np.uint8))

    # Transformations manuelles
    img_flp = img.transpose(Image.FLIP_LEFT_RIGHT)
    img_rot = img.rotate(15, resample=Image.BILINEAR)
    img_brt = ImageEnhance.Brightness(img).enhance(1.2)

    return [np.array(img), np.array(img_flp), np.array(img_rot), np.array(img_brt)]

augmented_images = []
augmented_labels = []

for i in range(len(X_train)):
    imgs_aug = augment_image(X_train[i])
    augmented_images.extend(imgs_aug)
    augmented_labels.extend([y_train[i]] * len(imgs_aug))

X_train_aug = np.array(augmented_images, dtype=np.uint8)
y_train_aug = np.array(augmented_labels, dtype=np.int64)

# Fusion avec les donnÃ©es originales
X_train = np.concatenate([X_train, X_train_aug], axis=0)
y_train = np.concatenate([y_train, y_train_aug], axis=0)
print("After offline augmentation:", X_train.shape)

# -----------------------------
# 2ï¸âƒ£ Normalisation et ajout du canal
# -----------------------------
X_train = X_train.astype("float32") / 255.0
X_val   = X_val.astype("float32") / 255.0
X_test  = X_test.astype("float32") / 255.0

if len(X_train.shape) == 3:
    X_train = np.expand_dims(X_train, -1)
    X_val   = np.expand_dims(X_val, -1)
    X_test  = np.expand_dims(X_test, -1)

# -----------------------------
# 3ï¸âƒ£ Augmentation ONLINE (dynamique)
# -----------------------------
datagen = ImageDataGenerator(
    rotation_range=10,       # rotations alÃ©atoires
    width_shift_range=0.05,  # petits dÃ©calages horizontaux
    height_shift_range=0.05, # petits dÃ©calages verticaux
    zoom_range=0.1,          # zoom lÃ©ger
    horizontal_flip=True,    # retournement horizontal
    fill_mode='nearest'      # remplissage des bords
)

# IMPORTANT : on "fit" le datagen sur X_train
datagen.fit(X_train)

print("âœ… Data augmentation hybride prÃªte !")
# ----------------------------
# Calcul des class weights
# ----------------------------
classes = np.unique(y_train)
weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)
class_weights = dict(enumerate(weights))
print("Class weights:", class_weights)

# ----------------------------
# DÃ©finition du modÃ¨le CNN
# ----------------------------
input_shape = X_train.shape[1:]

model = models.Sequential([
    layers.Conv2D(32, (3,3), activation='relu', input_shape=input_shape),
    layers.MaxPooling2D((2,2)),
    layers.Conv2D(64, (3,3), activation='relu'),
    layers.MaxPooling2D((2,2)),
    layers.Conv2D(128, (3,3), activation='relu'),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(1, activation='sigmoid')
])

# 8ï¸âƒ£ COMPILATION
# ===============================
model.compile(
    optimizer=Adam(learning_rate=1e-4),
    loss='binary_crossentropy',
    metrics=['accuracy']
)

model.summary()


# ----------------------------
# Callbacks
# ----------------------------
early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# ----------------------------
# EntraÃ®nement
# ----------------------------
history = model.fit(
    datagen.flow(X_train, y_train, batch_size=32),
    validation_data=(X_val, y_val),
    epochs=20,
    class_weight=class_weights,
    callbacks=[early_stop],
    verbose=1
)

# ----------------------------
# Ã‰valuation sur test

y_pred_prob = model.predict(X_test).ravel()  # forme (N,)
y_pred = (y_pred_prob > 0.5).astype(int)

# AUC
from sklearn import metrics
fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_prob)
auc_score = metrics.roc_auc_score(y_test, y_pred_prob)
print(f"AUC (CNN) : {auc_score:.3f}")


# Courbe ROC
plt.figure(figsize=(6,5))
plt.plot(fpr, tpr, label=f"AUC = {auc_score:.3f}", color='royalblue')
plt.plot([0,1], [0,1], 'k--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve â€” CNN")
plt.legend()
plt.show()

# Classification report et confusion matrix
cm = metrics.confusion_matrix(y_test, y_pred)
print("Confusion Matrix:\n", cm)
print(metrics.classification_report(y_test, y_pred, digits=3))

# ----------------------------
# PrÃ©paration des donnÃ©es
# ----------------------------


import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models, callbacks
from tensorflow.keras.optimizers import Adam
from sklearn.utils.class_weight import compute_class_weight
from sklearn import metrics
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from PIL import Image, ImageEnhance

# ----------------------------
# PrÃ©paration des donnÃ©es
# ----------------------------
X_train, y_train = h["train_images"], h["train_labels"].ravel()
X_val, y_val     = h["val_images"], h["val_labels"].ravel()
X_test, y_test   = h["test_images"], h["test_labels"].ravel()

y_train = y_train.astype(int).ravel()
y_val   = y_val.astype(int).ravel()
y_test  = y_test.astype(int).ravel()


# -----------------------------
# 1ï¸âƒ£ Augmentation OFFLINE (statique)
# -----------------------------
def augment_image(img_array):
    """Applique plusieurs transformations sur une image (PIL)."""
    img = Image.fromarray(img_array.astype(np.uint8))

    # Transformations manuelles
    img_flp = img.transpose(Image.FLIP_LEFT_RIGHT)
    img_rot = img.rotate(15, resample=Image.BILINEAR)
    img_brt = ImageEnhance.Brightness(img).enhance(1.2)

    return [np.array(img), np.array(img_flp), np.array(img_rot), np.array(img_brt)]

augmented_images = []
augmented_labels = []

for i in range(len(X_train)):
    imgs_aug = augment_image(X_train[i])
    augmented_images.extend(imgs_aug)
    augmented_labels.extend([y_train[i]] * len(imgs_aug))

X_train_aug = np.array(augmented_images, dtype=np.uint8)
y_train_aug = np.array(augmented_labels, dtype=np.int64)

# Fusion avec les donnÃ©es originales
X_train = np.concatenate([X_train, X_train_aug], axis=0)
y_train = np.concatenate([y_train, y_train_aug], axis=0)
print("After offline augmentation:", X_train.shape)

# -----------------------------
# 2ï¸âƒ£ Normalisation et ajout du canal
# -----------------------------
X_train = X_train.astype("float32") / 255.0
X_val   = X_val.astype("float32") / 255.0
X_test  = X_test.astype("float32") / 255.0

if len(X_train.shape) == 3:
    X_train = np.expand_dims(X_train, -1)
    X_val   = np.expand_dims(X_val, -1)
    X_test  = np.expand_dims(X_test, -1)

# -----------------------------
# 3ï¸âƒ£ Augmentation ONLINE (dynamique)
# -----------------------------
datagen = ImageDataGenerator(
    rotation_range=10,       # rotations alÃ©atoires
    width_shift_range=0.05,  # petits dÃ©calages horizontaux
    height_shift_range=0.05, # petits dÃ©calages verticaux
    zoom_range=0.1,          # zoom lÃ©ger
    horizontal_flip=True,    # retournement horizontal
    fill_mode='nearest'      # remplissage des bords
)

# IMPORTANT : on "fit" le datagen sur X_train
datagen.fit(X_train)

print("âœ… Data augmentation hybride prÃªte !")
# ----------------------------
# Calcul des class weights
# ----------------------------
classes = np.unique(y_train)
weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)
class_weights = dict(enumerate(weights))
print("Class weights:", class_weights)

# ----------------------------
# DÃ©finition du modÃ¨le CNN
# ----------------------------
input_shape = X_train.shape[1:]

model = models.Sequential([
    layers.Conv2D(32, (3,3), activation='relu', input_shape=input_shape),
    layers.MaxPooling2D((2,2)),
    layers.Conv2D(64, (3,3), activation='relu'),
    layers.MaxPooling2D((2,2)),
    layers.Conv2D(128, (3,3), activation='relu'),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(1, activation='sigmoid')
])

# 8ï¸âƒ£ COMPILATION
# ===============================
model.compile(
    optimizer=Adam(learning_rate=1e-4),
    loss='binary_crossentropy',
    metrics=['accuracy']
)

model.summary()


# ----------------------------
# Callbacks
# ----------------------------
early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# ----------------------------
# EntraÃ®nement
# ----------------------------
history = model.fit(
    datagen.flow(X_train, y_train, batch_size=32),
    validation_data=(X_val, y_val),
    epochs=20,
    class_weight=class_weights,
    callbacks=[early_stop],
    verbose=1
)

# ----------------------------
# Ã‰valuation sur test

y_pred_prob = model.predict(X_test).ravel()  # forme (N,)
y_pred = (y_pred_prob > 0.5).astype(int)

# AUC
from sklearn import metrics
fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_prob)
auc_score = metrics.roc_auc_score(y_test, y_pred_prob)
print(f"AUC (CNN) : {auc_score:.3f}")


# Courbe ROC
plt.figure(figsize=(6,5))
plt.plot(fpr, tpr, label=f"AUC = {auc_score:.3f}", color='royalblue')
plt.plot([0,1], [0,1], 'k--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve â€” CNN")
plt.legend()
plt.show()

# Classification report et confusion matrix
cm = metrics.confusion_matrix(y_test, y_pred)
print("Confusion Matrix:\n", cm)
print(metrics.classification_report(y_test, y_pred, digits=3))

