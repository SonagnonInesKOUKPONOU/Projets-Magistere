# -*- coding: utf-8 -*-
"""Project_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vRN_hHL04_AlKoiAoGnV5ziBfnys9B3a
"""

!pip -q install -U "numpy==1.26.4" "scipy==1.11.4"

!pip install -q --no-deps gensim==4.3.3

!pip install -U bitsandbytes

!pip install transformers

import os; os.kill(os.getpid(), 9)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
import re #regular expression
import nltk # natural language tool kit
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from wordcloud import WordCloud
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_recall_fscore_support,roc_curve, roc_auc_score
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
import warnings
warnings.filterwarnings("ignore")
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import torch
nltk.download('punkt_tab')
nltk.download('stopwords')
stemmer = PorterStemmer()

train_data = pd.read_csv("train.csv")
print (train_data.head())

#keeping only the useful columns
train_data = train_data[["comment_text", "toxic"]]
pd.set_option("display.max_colwidth", None)
train_data

"""## Part 1 : Data exploration and preprocessing

Because, we want to apply different strategy from the previous project, we will apply same preprocessing step but it will be minimal.
"""

# Step 1: Tokenization  (word split)
train_data["tokens"] = train_data["comment_text"].apply(lambda x: word_tokenize(str(x)))

stop_words = set(stopwords.words('english'))
# Step 2: Remove stopwords

stop_words -= {
    "not", "no", "don", "don't", "wasn", "wasn't",
    "isn", "isn't", "hasn", "hasn't", "haven", "haven't",
    "hadn", "hadn't", "won", "won't", "wouldn", "wouldn't",
    "shouldn", "shouldn't", "mustn", "mustn't",
    "needn", "needn't", "shan", "shan't",
    "mightn", "mightn't", "couldn", "couldn't",
    "aren", "aren't", "you", "out", "up"
}

train_data["tokens_nostop"] = train_data["tokens"].apply(
    lambda toks: [t for t in toks if t not in stop_words]
)

#Step 3 : Remove punctuation
def clean_tokens(tokens):
    cleaned = []
    for t in tokens:
        t = re.sub(r"<*,?>", " ", t)  # remove HTML tags like <br />
        t = re.sub(r"\s+", " ", t).strip()   # Remove excessive whitespace
        t = re.sub(r"&[a-z]+;", " ", t)   # remove HTML entities like &nbsp; &amp; &quot;
        t = re.sub(r"[^a-zA-Z'!]", '', t)      # keep only alphabetic characters
        if t and t not in ["br", "jpg", "png", "http", "www", "com"]: # drop empty tokens and special terms
            cleaned.append(t)
    return cleaned

# Our data with no ponctuaction
train_data["tokens_nopunc"] = train_data["tokens_nostop"].apply(clean_tokens)

# Step 4 : Join words
train_data["processed_text"] = train_data["tokens_nopunc"].apply(lambda toks: " ".join(toks))

"""### Part 1 : Baseline model
From the first project, we found that our best was the combination : Trigram + TF IDF + LinearSVC
"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import roc_auc_score


vectorizer_tfidf = TfidfVectorizer(ngram_range=(1,3))

X_tfidf = vectorizer_tfidf.fit_transform(train_data["processed_text"])

y = train_data["toxic"].astype(int)
X_train, X_test, y_train, y_test = train_test_split(
    X_tfidf, y, test_size=0.2, random_state=42, stratify=y)
print("train size:", X_train.shape, " | test size:", X_test.shape)


# Evaluating the machine learning approach (SVM model)
from sklearn.svm import LinearSVC

clf_svm = LinearSVC(loss="squared_hinge", random_state=42)
clf_svm.fit(X_train, y_train)

y_pred_test_svm = clf_svm.predict(X_test)

print("TF-IDF + LinearSVC â€” classification report")
print(classification_report(y_test, y_pred_test_svm, target_names=["safe","toxic"]))

cm_svm = confusion_matrix(y_test, y_pred_test_svm)
print(pd.DataFrame(cm_svm, index=["true_safe","true_toxic"], columns=["pred_safe","pred_toxic"]))

y_scores_svm = clf_svm.decision_function(X_test)
auc_svm = roc_auc_score(y_test, y_scores_svm)
print(f"AUC Linear SVM = {auc_svm:.2f}")

"""## Part 2 : Embeddings
In this part, we will use different embeddings. Ths first one will be word embedding with fastText and the second one will be sentence embedding with Mini LM as model.

__Reminder__ : For the embedding and the next step, we will use the complete dataset without processing steps.


"""

# =======================
# Data Split
# =======================
X = train_data["comment_text"].astype(str).values
y = train_data["toxic"].astype(int).values
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

"""### Part 2-A : Word embedding

"""

import gensim.downloader as api

# ============
# FASTTEXT EMBEDDINGS
# ============
ft = api.load("fasttext-wiki-news-subwords-300")

def doc_vector_fasttext(doc):
    tokens = doc.split()
    vecs = [ft[w] for w in tokens if w in ft]
    if not vecs:
        return np.zeros(ft.vector_size)
    return np.mean(vecs, axis=0)

X_train_ft = np.vstack([doc_vector_fasttext(doc) for doc in X_train])
X_test_ft  = np.vstack([doc_vector_fasttext(doc) for doc in X_test])

# ============
# 3. LOGISTIC REGRESSION + FastText
# ============
clf_log_ft = LogisticRegression(max_iter=2000, class_weight="balanced")
clf_log_ft.fit(X_train_ft, y_train)

y_pred_log_ft = clf_log_ft.predict(X_test_ft)
y_proba_log_ft = clf_log_ft.predict_proba(X_test_ft)[:, 1]

print("[Logistic Regression - FastText]")
print(classification_report(y_test, y_pred_log_ft))
print("AUC:", roc_auc_score(y_test, y_proba_log_ft))

# ============
# SVM CLASSIFIER + FastText
# ============
from sklearn.svm import LinearSVC
clf_svm_ft = LinearSVC(loss="squared_hinge", random_state=42)
clf_svm_ft.fit(X_train_ft, y_train)

y_pred_svm_ft = clf_svm_ft.predict(X_test_ft)

print("\n[SVM - FastText]")
y_scores_svm = clf_svm_ft.decision_function(X_test_ft)
auc_svm = roc_auc_score(y_test, y_scores_svm)
print("TF-IDF + LinearSVC â€” classification report")
print(classification_report(y_test, y_pred_svm_ft, target_names=["safe","toxic"]))
cm_svm = confusion_matrix(y_test, y_pred_svm_ft)
print(pd.DataFrame(cm_svm, index=["true_safe","true_toxic"], columns=["pred_safe","pred_toxic"]))
print(f"AUC Linear SVM = {auc_svm:.3f}")

"""### Part 2-B : Sentence embedding"""

from sentence_transformers import SentenceTransformer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, roc_auc_score
from sklearn.model_selection import train_test_split
import time


# =======================
# Sentence Embeddings
# =======================
miniLM = SentenceTransformer("all-MiniLM-L6-v2")

t0 = time.time()
X_train_emb = miniLM.encode(list(X_train), show_progress_bar=True, batch_size=64)
X_test_emb  = miniLM.encode(list(X_test), show_progress_bar=True, batch_size=64)
t_encode = time.time() - t0

print("miniLM encoding done in", round(t_encode, 2), "s")

# =============================
# Logistic Regression + MiniLM
# =============================
t0 = time.time()
clf_log = LogisticRegression(max_iter=2000, class_weight="balanced")
clf_log.fit(X_train_emb, y_train)
y_proba_log = clf_log.predict_proba(X_test_emb)[:, 1]
y_pred_log = clf_log.predict(X_test_emb)
t_log = time.time() - t0

print("\n=== Logistic Regression ===")
print("Training + inference time:", round(t_log, 2), "s")
print(classification_report(y_test, y_pred_log, target_names=["SAFE","TOXIC"]))
print("AUC:", roc_auc_score(y_test, y_proba_log))

# =======================
# SVM (linÃ©aire) + miniLM
# =======================
from sklearn.svm import LinearSVC
t0 = time.time()
clf_svm = LinearSVC(loss="squared_hinge", random_state=42)
clf_svm.fit(X_train_emb, y_train)
y_pred_svm = clf_svm.predict(X_test_emb)
t_svm = time.time() - t0

print("\n=== SVM ===")
print("Training + inference time:", round(t_svm, 2), "s")
print(classification_report(y_test, y_pred_svm, target_names=["SAFE","TOXIC"]))
y_scores_svm_emb = clf_svm.decision_function(X_test_emb)
auc_svm = roc_auc_score(y_test, y_scores_svm_emb)
print(f"AUC Linear SVM + miniLM = {auc_svm:.3f}")

"""## Part 3 Using transformers model

We will use a pretrained transformer model via hugging face pipeline which is toxic bert.

The model based on transformers have already the own process of tokenisation and preprocessing data. So, we don't need to preprocess step on the data before using transformers models
"""

# ==============================
# Toxic-BERT
# ==============================
from transformers import pipeline

toxic_classifier = pipeline(
    "text-classification",
    model="unitary/toxic-bert",
    tokenizer="unitary/toxic-bert",
    return_all_scores=True,
    truncation=True,
    max_length=512,
    device=0,         # utilise GPU si dispo, sinon CPU
    batch_size=32     # âœ… traitement en batch pour accÃ©lÃ©rer
)

def evaluate_with_toxicbert(X_test, y_test):
    t0 = time.time()

    # Inference directe par batch
    preds = toxic_classifier(list(X_test), batch_size=32)

    y_pred, y_score = [], []
    for p in preds:
        score_dict = {d['label'].lower(): d['score'] for d in p}
        label = 1 if "toxic" in score_dict and score_dict["toxic"] > 0.5 else 0
        conf = score_dict.get("toxic", 0.5)
        y_pred.append(label)
        y_score.append(conf)

    y_pred = np.array(y_pred)
    y_score = np.array(y_score)

    t1 = time.time()

    print("\n=== Toxic-BERT ===")
    print(classification_report(y_test, y_pred, target_names=["SAFE","TOXIC"]))
    print("AUC:", roc_auc_score(y_test, y_score))
    print("Inference time:", round(t1 - t0, 2), "s")

    # Confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    print("Confusion Matrix:\n", pd.DataFrame(cm,
                                              index=["true_SAFE","true_TOXIC"],
                                              columns=["pred_SAFE","pred_TOXIC"]))


# âœ… Ã‰valuer sur toute la base test
evaluate_with_toxicbert(X_test, y_test)

"""### Part 3 : Prompt engineering"""

# ============================================================
# Prompt Engineering avec Flan-T5-Large
# ============================================================

import time
import re
from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM

# Charger le modÃ¨le Flan-T5-Large
model_id = "google/flan-t5-large"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForSeq2SeqLM.from_pretrained(
    model_id,
    device_map="auto",       # GPU si dispo
    torch_dtype="auto",      # bfloat16/float16 si dispo
    load_in_8bit=True        # âš¡ quantization pour rÃ©duire la RAM GPU
)
llm = pipeline("text2text-generation",
    model=model,
    tokenizer=tokenizer,
    device_map="auto",
    truncation=True,
    max_length=512,
    do_sample=False,
    temperature=0.1,
    num_beams=1
)
# =======================
# 1. Enhanced Prompts with Chain of Thought
# =======================
def prompt_zero_shot_cot(comment):
    return f"""Analyze this comment step by step and classify it as TOXIC or SAFE.

Comment: "{comment}"

Step-by-step reasoning:
1. Identify the tone and language used:
2. Look for toxic elements (insults, hate speech, harassment):
3. Evaluate the intention behind the message:
4. Final conclusion:

Classification:"""

def prompt_role_cot(comment):
    return f"""You are an expert content moderator with 10 years of experience.
Analyze this comment thoroughly using chain of thought.

COMMENT: "{comment}"

DETAILED ANALYSIS PROCEDURE:
[THOUGHT 1] Lexical analysis: What words or phrases could be problematic?
[THOUGHT 2] Contextual analysis: What is the context and intention?
[THOUGHT 3] Potential impact: Could this comment harm someone?
[THOUGHT 4] Guidelines comparison: Does it violate content policies?

FINAL CLASSIFICATION:"""

def prompt_few_shot_cot(comment):
    examples = """Examples with reasoning:

Comment: "You are completely stupid and incompetent"
Reasoning: Contains insult "stupid" and personal attack "incompetent". Aggressive tone.
Classification: TOXIC

Comment: "I disagree but I respect your opinion"
Reasoning: Polite expression of disagreement, no insults, respectful tone.
Classification: SAFE

Comment: "People like you shouldn't have the right to speak"
Reasoning: Excluding speech, dehumanizing language, intention to silence.
Classification: TOXIC

Comment: "Thanks for sharing this interesting perspective"
Reasoning: Positive reinforcement, appreciative language, constructive.
Classification: SAFE

"""
    return f"{examples}\nNow analyze this comment:\nComment: {comment}\nReasoning:"

# =======================
# 2. Enhanced Classification
# =======================
def classify_with_prompt(text, style="zero"):
    if style == "zero":
        prompt = prompt_zero_shot_cot(text)
    elif style == "role":
        prompt = prompt_role_cot(text)
    elif style == "few":
        prompt = prompt_few_shot_cot(text)
    else:
        raise ValueError("style must be 'zero', 'role' or 'few'")

    out = llm(prompt, max_new_tokens=100, do_sample=False)[0]["generated_text"]

    # Enhanced parsing with confidence scoring
    out_upper = out.upper()

    # Check for explicit classification
    if "TOXIC" in out_upper:
        toxic_confidence = 0.9
        # Adjust confidence based on reasoning clarity
        if "REASONING" in out_upper or "THOUGHT" in out_upper or "ANALYSIS" in out_upper:
            toxic_confidence = 0.95
        return 1, toxic_confidence
    elif "SAFE" in out_upper:
        safe_confidence = 0.9
        if "REASONING" in out_upper or "THOUGHT" in out_upper or "ANALYSIS" in out_upper:
            safe_confidence = 0.95
        return 0, safe_confidence
    else:
        # Fallback: look for toxic indicators in the reasoning
        toxic_indicators = ["INSULT", "HATE", "HARASSMENT", "AGGRESSIVE", "ATTACK", "HARMFUL"]
        safe_indicators = ["RESPECT", "POLITE", "CONSTRUCTIVE", "POSITIVE", "NEUTRAL"]

        toxic_count = sum(1 for indicator in toxic_indicators if indicator in out_upper)
        safe_count = sum(1 for indicator in safe_indicators if indicator in out_upper)

        if toxic_count > safe_count:
            return 1, 0.7  # Moderate confidence
        elif safe_count > toxic_count:
            return 0, 0.7
        else:
            return 0, 0.5  # Default to safe with low confidence

# =======================
# 3. Comprehensive Evaluation Function
# =======================
def evaluate_flan(X_test, y_test, style="zero", n_samples=500):
    """
    Enhanced evaluation with comprehensive metrics including AUC and classification report
    """
    print(f"\n{'='*60}")
    print(f"Evaluating {style.upper()} prompt style with {n_samples} samples")
    print(f"{'='*60}")

    # Sample data if needed
    if n_samples < len(X_test):
        indices = np.random.choice(len(X_test), n_samples, replace=False)
        X_sampled = [X_test[i] for i in indices]
        y_sampled = [y_test[i] for i in indices]
    else:
        X_sampled = X_test
        y_sampled = y_test

    predictions = []
    confidences = []
    reasoning_texts = []

    start_time = time.time()

    for i, comment in enumerate(X_sampled):
        if i % 50 == 0:
            print(f"Processing sample {i}/{len(X_sampled)}...")

        pred, conf = classify_with_prompt(comment, style=style)
        predictions.append(pred)
        confidences.append(conf)

        # Store reasoning for analysis (optional)
        reasoning_texts.append(f"Pred: {pred}, Conf: {conf:.2f}")

    total_time = time.time() - start_time

    # Calculate metrics
    accuracy = np.mean(np.array(predictions) == np.array(y_sampled))

    # AUC Score
    try:
        auc_score = roc_auc_score(y_sampled, confidences)
    except Exception as e:
        print(f"Could not calculate AUC: {e}")
        auc_score = 0.5

    # Classification Report
    class_report = classification_report(y_sampled, predictions, target_names=['SAFE', 'TOXIC'])

    # Confusion Matrix
    cm = confusion_matrix(y_sampled, predictions)

    # Display Results
    print(f"\nðŸ“Š RESULTS for {style.upper()} prompt:")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"AUC Score: {auc_score:.4f}")
    print(f"Total Time: {total_time:.2f}s")
    print(f"Speed: {len(X_sampled)/total_time:.2f} samples/sec")

    print(f"\nConfusion Matrix:")
    print(cm)

    print(f"\nClassification Report:")
    print(class_report)

    # Detailed performance analysis
    safe_mask = np.array(y_sampled) == 0
    toxic_mask = np.array(y_sampled) == 1

    safe_accuracy = np.mean(np.array(predictions)[safe_mask] == 0) if np.sum(safe_mask) > 0 else 0
    toxic_accuracy = np.mean(np.array(predictions)[toxic_mask] == 1) if np.sum(toxic_mask) > 0 else 0

    print(f"\nDetailed Performance:")
    print(f"SAFE comments accuracy: {safe_accuracy:.4f} ({np.sum(safe_mask)} samples)")
    print(f"TOXIC comments accuracy: {toxic_accuracy:.4f} ({np.sum(toxic_mask)} samples)")

    return {
        'predictions': predictions,
        'confidences': confidences,
        'accuracy': accuracy,
        'auc_score': auc_score,
        'confusion_matrix': cm,
        'classification_report': class_report,
        'total_time': total_time
    }

# =======================
# 4. Comparative Analysis
# =======================
def compare_prompt_strategies(X_test, y_test, n_samples=500):
    """
    Compare all three prompt strategies and provide recommendations
    """
    results = {}

    for style in ['zero', 'role', 'few']:
        results[style] = evaluate_flan(X_test, y_test, style=style, n_samples=n_samples)

    # Comparative analysis
    print(f"\n{'='*80}")
    print("COMPARATIVE ANALYSIS OF PROMPT STRATEGIES")
    print(f"{'='*80}")

    comparison_data = []
    for style in ['zero', 'role', 'few']:
        comparison_data.append({
            'Style': style.upper(),
            'Accuracy': results[style]['accuracy'],
            'AUC Score': results[style]['auc_score'],
            'Time (s)': results[style]['total_time']
        })

    comparison_df = pd.DataFrame(comparison_data)
    print(comparison_df.to_string(index=False))

    # Find best performing strategy
    best_accuracy = max(comparison_df['Accuracy'])
    best_auc = max(comparison_df['AUC Score'])

    best_accuracy_style = comparison_df[comparison_df['Accuracy'] == best_accuracy]['Style'].iloc[0]
    best_auc_style = comparison_df[comparison_df['AUC Score'] == best_auc]['Style'].iloc[0]

    print(f"\nðŸŽ¯ RECOMMENDATIONS:")
    print(f"Best accuracy: {best_accuracy_style} prompt ({best_accuracy:.4f})")
    print(f"Best AUC score: {best_auc_style} prompt ({best_auc:.4f})")

    return results, comparison_df

# =======================
# 5. Usage Example
# =======================
if __name__ == "__main__":
    # Evaluate individual prompts
    print("Testing individual prompt strategies...")
    zero_results = evaluate_flan(X_test, y_test, style="zero", n_samples=500)
    role_results = evaluate_flan(X_test, y_test, style="role", n_samples=500)
    few_results = evaluate_flan(X_test, y_test, style="few", n_samples=500)

    # Comparative analysis
    all_results, comparison_df = compare_prompt_strategies(X_test, y_test, n_samples=len(X_test))

    print(f"\nâœ… Evaluation complete! Use the best performing prompt for your application.")